{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with Spark\n",
    "\n",
    "Now that we have a brief idea of Spark and SQLContext, we are ready to build our first Machine learning program.\n",
    "we will proceed as follow:\n",
    "\n",
    "    Step 1) Basic operation with PySpark\n",
    "    Step 2) Data preprocessing\n",
    "    Step 3) Build a data processing pipeline\n",
    "    Step 4) Build the classifier\n",
    "    Step 5) Train and evaluate the model\n",
    "    Step 6) Tune the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.getOrCreate()#create spark session \n",
    "sc = spark.sparkContext#create sparkContext\n",
    "from pyspark.sql.types import  (StructType, \n",
    "                                StructField, \n",
    "                                DateType, \n",
    "                                BooleanType,\n",
    "                                DoubleType,\n",
    "                                IntegerType,\n",
    "                                StringType,\n",
    "                                DecimalType,\n",
    "                                LongType,\n",
    "                                ArrayType,\n",
    "                                TimestampType)\n",
    "sample_schema = StructType([StructField(\"id\",  IntegerType(), True),\n",
    "                            StructField(\"timeAtServer\", DoubleType(), True),\n",
    "                            StructField(\"aircraft\",  IntegerType(), True ),\n",
    "                            StructField(\"latitude\", DoubleType(), True),\n",
    "                            StructField(\"longitude\", DoubleType(), True  ),\n",
    "                            StructField(\"baroAltitude\", DoubleType(), True ),\n",
    "                            StructField(\"geoAltitude\", DoubleType(), True ),\n",
    "                            StructField(\"numM\",IntegerType(), True),\n",
    "                            StructField(\"measurements\", StringType(), True)           \n",
    "                            ])\n",
    "sample_aircarft = spark.read.csv(\"C:/PFE/TEST/training_1_category_1.csv\",\n",
    "                       header = True, \n",
    "                        schema = sample_schema)\n",
    "#filter only right data :\n",
    "sample_aircarft_filtred=sample_aircarft.filter((sample_aircarft.latitude !=0)\n",
    "                                               |(sample_aircarft.longitude != 0)\n",
    "                                               | (sample_aircarft.baroAltitude != 0)\n",
    "                                               | (sample_aircarft.geoAltitude!= 0)\n",
    "                                               | (sample_aircarft.geoAltitude!= 'null')\n",
    "                                               | (sample_aircarft.measurements != 'NA'))\n",
    "sample_aircarft =sample_aircarft.orderBy('aircraft','timeAtServer')\n",
    "sensors_schema = StructType([StructField(\"serial\", LongType(), True),\n",
    "                            StructField(\"latitudes\", DoubleType(), True ),\n",
    "                            StructField(\"longitudes\",DoubleType(), True),\n",
    "                            StructField(\"height\",DoubleType(), True),\n",
    "                            StructField(\"type\", StringType(), True)\n",
    "                            ])\n",
    "sensors = spark.read.csv(\"C:/PFE/TEST/sensors.csv\",\n",
    "                       header = True, \n",
    "                        schema = sensors_schema)\n",
    "sample_aircarft.na.drop(how = 'all')\n",
    "sample_aircarft.dropDuplicates()\n",
    "sensors.na.drop(how = 'all')\n",
    "sensors.dropDuplicates()\n",
    "sensors_filtred=sensors.filter((sensors.latitudes !=0) & (sensors.longitudes != 0))\n",
    "#let's check our work !\n",
    "sample_aircarft_filtred.filter((sample_aircarft_filtred[\"latitude\"] == \"\") | sample_aircarft_filtred[\"latitude\"].isNull() | isnan(sample_aircarft_filtred[\"latitude\"])).count()\n",
    "#Amazing  we have 0 null , nan  values !!!! #eleminating null data \n",
    "sample_aircarft.na.drop(how = 'all')\n",
    "sample_aircarft.dropDuplicates()\n",
    "from pyspark.sql.functions import *\n",
    "sample_aircarft_filtred = sample_aircarft_filtred.withColumn(\"ArrayOfString\",\n",
    "                                                             split(col(\"measurements\"), \"\\],\\s*\\[\")\n",
    "                                                             .cast(ArrayType(StringType()))\n",
    "                                                             .alias(\"ArrayOfString\"))                                           \n",
    "sample_aircarft_filtred = sample_aircarft_filtred.withColumn(\"sensors_informations\",\n",
    "                                                             explode_outer('ArrayOfString'))\n",
    "sample_aircarft_filtred=sample_aircarft_filtred.withColumn(\"sensors_informations\", \n",
    "                                                           regexp_replace(col(\"sensors_informations\"), \"[\\\\[\\\\]]\", \"\"))\n",
    "sample_aircarft_filtred=sample_aircarft_filtred.withColumn(\"sensors_informations\",\n",
    "                                                           split(col(\"sensors_informations\"), \",\\s*\")\n",
    "                                                           .cast(ArrayType(IntegerType())).alias(\"sensors informations\"))\n",
    "sample_aircarft_filtred = sample_aircarft_filtred.withColumn(\"serial_F\",\n",
    "                                                             sample_aircarft_filtred[\"sensors_informations\"]\n",
    "                                                             .getItem(0).cast(DoubleType()))\n",
    "sample_aircarft_filtred = sample_aircarft_filtred.withColumn(\"timestamp\",\n",
    "                                                             sample_aircarft_filtred[\"sensors_informations\"]\n",
    "                                                             .getItem(1).cast(LongType()))\n",
    "sample_aircarft_filtred= sample_aircarft_filtred.withColumn(\"signalstrength\",\n",
    "                                                            sample_aircarft_filtred[\"sensors_informations\"]\n",
    "                                                            .getItem(2).cast(DoubleType()))\n",
    "#==>Data integration: Using multiple databases\n",
    "#spatial join sesnors data + aircrafts data , i used inner join \n",
    "joined_aircarft_sensors=sample_aircarft_filtred.join(sensors_filtred,\n",
    "                                                     sample_aircarft_filtred.serial_F == sensors_filtred.serial,\n",
    "                                                     'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regourp our sensors informations from joined data in the same row  : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+--------+---------+----------+--------------+---------+-------+----------------+----------------+\n",
      "|aircraft|timeAtServer|baroAltitude|serial_F|latitudes|longitudes|signalstrength|timestamp|height |latitude        |longitude       |\n",
      "+--------+------------+------------+--------+---------+----------+--------------+---------+-------+----------------+----------------+\n",
      "|1787    |0.0         |6400.8      |463.0   |49.471601|7.696532  |4.0           |963309455|273.985|49.5238952636719|7.80282271535773|\n",
      "|1787    |0.0         |6400.8      |424.0   |49.42498 |7.75332   |27.0          |963315122|277.015|49.5238952636719|7.80282271535773|\n",
      "|1787    |0.0         |6400.8      |412.0   |49.287572|7.603982  |0.0           |963373222|410.652|49.5238952636719|7.80282271535773|\n",
      "+--------+------------+------------+--------+---------+----------+--------------+---------+-------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_aircarft_sensors.select('aircraft',\"timeAtServer\",'baroAltitude','serial_F',\n",
    "                               'latitudes','longitudes','signalstrength',\n",
    "                               'timestamp','height','latitude','longitude').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-method using 'ARRAY' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "joined_aircarft_sensors =joined_aircarft_sensors.orderBy('aircraft','timeAtServer')\n",
    "columns = [F.col(\"serial_F\"),F.col(\"latitudes\"),\n",
    "           F.col(\"longitudes\"),F.col(\"signalstrength\"),\n",
    "            F.col(\"height\")] \n",
    "Grouped_aircarft_sensors = joined_aircarft_sensors.withColumn('sensor_infos',F.array(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+------------------------------------------+----------------+----------------+\n",
      "|aircraft|timeAtServer|baroAltitude|sensor_infos                              |latitude        |longitude       |\n",
      "+--------+------------+------------+------------------------------------------+----------------+----------------+\n",
      "|1787    |0.0         |6400.8      |[463.0, 49.471601, 7.696532, 4.0, 273.985]|49.5238952636719|7.80282271535773|\n",
      "|1787    |0.0         |6400.8      |[412.0, 49.287572, 7.603982, 0.0, 410.652]|49.5238952636719|7.80282271535773|\n",
      "|1787    |0.0         |6400.8      |[424.0, 49.42498, 7.75332, 27.0, 277.015] |49.5238952636719|7.80282271535773|\n",
      "+--------+------------+------------+------------------------------------------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Grouped_aircarft_sensors.filter('aircraft=1787').select('aircraft','timeAtServer','baroAltitude',\n",
    "                                'sensor_infos','latitude','longitude').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Group all sensors informations from all stations using 'collect_list':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.orderBy('aircraft','timeAtServer')\n",
    "my_window = Window.partitionBy('aircraft','timeAtServer').orderBy(\"timeAtServer\")\n",
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.withColumn('All_sensor_infos', \n",
    "                                                            collect_list('sensor_infos'              \n",
    "                                                            ).over(my_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "|baroAltitude|All_sensor_infos                                                                                                                   |latitude        |longitude       |\n",
      "+------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "|6400.8      |[[463.0, 49.471601, 7.696532, 4.0, 273.985], [424.0, 49.42498, 7.75332, 27.0, 277.015], [412.0, 49.287572, 7.603982, 0.0, 410.652]]|49.5238952636719|7.80282271535773|\n",
      "|6400.8      |[[463.0, 49.471601, 7.696532, 4.0, 273.985], [424.0, 49.42498, 7.75332, 27.0, 277.015], [412.0, 49.287572, 7.603982, 0.0, 410.652]]|49.5238952636719|7.80282271535773|\n",
      "|6400.8      |[[463.0, 49.471601, 7.696532, 4.0, 273.985], [424.0, 49.42498, 7.75332, 27.0, 277.015], [412.0, 49.287572, 7.603982, 0.0, 410.652]]|49.5238952636719|7.80282271535773|\n",
      "+------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Grouped_aircarft_sensors.filter('aircraft=1787').select('baroAltitude','All_sensor_infos','latitude','longitude').show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Removing duplicates row: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.dropDuplicates(['aircraft', 'timeAtServer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "|baroAltitude|All_sensor_infos                                                                                                                                                                                                                                                                                                              |latitude        |longitude       |\n",
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "|6400.8      |[[463.0, 49.471601, 7.696532, 4.0, 273.985], [424.0, 49.42498, 7.75332, 27.0, 277.015], [412.0, 49.287572, 7.603982, 0.0, 410.652]]                                                                                                                                                                                           |49.5238952636719|7.80282271535773|\n",
      "|6400.8      |[[320.0, 47.347179, 8.641771, 61.0, 658.186853], [424.0, 49.42498, 7.75332, 25.0, 277.015], [412.0, 49.287572, 7.603982, 33.0, 410.652], [402.0, 49.471547, 7.696765, 44.0, 261.548]]                                                                                                                                         |49.5234375      |7.80390689247533|\n",
      "|6400.8      |[[440.0, 50.048573, 8.487899, 66.0, 82.381], [402.0, 49.471547, 7.696765, 40.0, 261.548], [320.0, 47.347179, 8.641771, 69.0, 658.186853], [463.0, 49.471601, 7.696532, 29.0, 273.985], [424.0, 49.42498, 7.75332, 15.0, 277.015], [115.0, 50.937081, 7.353797, 37.0, 262.311051], [412.0, 49.287572, 7.603982, 15.0, 410.652]]|49.5231085308528|7.80482111750422|\n",
      "+------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Grouped_aircarft_sensors.filter('aircraft=1787').select('baroAltitude','All_sensor_infos','latitude','longitude').show(3, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(longitude AS DOUBLE)'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geohash2\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "#to create geohash positions we  need float num (lat, long)\n",
    "Grouped_aircarft_sensors.baroAltitude.cast(DoubleType())\n",
    "Grouped_aircarft_sensors.latitude.cast(DoubleType()) \n",
    "Grouped_aircarft_sensors.longitude.cast(DoubleType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create geohash positions we  need float num (lat, long)\n",
    "#we will create geohash with precision of  4 bit , error +20 km,-20km\n",
    "def eval_results(x,y):\n",
    "    try:\n",
    "        return geohash2.encode(x,y,precision=4)\n",
    "    except:\n",
    "        return (None)\n",
    "udf1 = F.udf(eval_results,StringType())\n",
    "udf2= F.udf(lambda x,y,z,t: geodesic((x,y),(z,t)))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.withColumn('infos_Flatten',\n",
    "                                                             flatten(Grouped_aircarft_sensors.All_sensor_infos).alias('infos_Flatten'))\n",
    "# will pad our sequaences of data , max =13 * 5 =65 senors in our data \n",
    "pad_fix_length = F.udf(\n",
    "    lambda arr: arr[:65] + [0] * (65 - len(arr[:65])), \n",
    "    ArrayType(DoubleType())\n",
    ")\n",
    "\n",
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.withColumn('infos_Flatten_pad',\n",
    "                                                             pad_fix_length(Grouped_aircarft_sensors.infos_Flatten).alias('infos_Flatten_pad'))\n",
    "\n",
    "# last thing convert our array to vectir \n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.withColumn('VectorUDT', list_to_vector_udf('infos_Flatten_pad').alias(\"VectorUDT\"))\n",
    "Grouped_aircarft_sensors=Grouped_aircarft_sensors.withColumn('Geohash_aircraft', udf1('latitude','longitude').cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- timeAtServer: double (nullable = true)\n",
      " |-- aircraft: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- baroAltitude: double (nullable = true)\n",
      " |-- geoAltitude: double (nullable = true)\n",
      " |-- numM: integer (nullable = true)\n",
      " |-- measurements: string (nullable = true)\n",
      " |-- ArrayOfString: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- sensors_informations: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- serial_F: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- signalstrength: double (nullable = true)\n",
      " |-- serial: long (nullable = true)\n",
      " |-- latitudes: double (nullable = true)\n",
      " |-- longitudes: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- sensor_infos: array (nullable = false)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- All_sensor_infos: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |-- infos_Flatten: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- infos_Flatten_pad: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- VectorUDT: vector (nullable = true)\n",
      " |-- Geohash_aircraft: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Grouped_aircarft_sensors.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Geoash_aircraft class:\n",
    "   ### Feature Engineering in pyspark \n",
    "The most commonly used data pre-processing techniques in approaches in Spark are as follows\n",
    "\n",
    " 1) VectorAssembler\n",
    "\n",
    " 2)Bucketing\n",
    "\n",
    " 3)Scaling and normalization\n",
    "\n",
    " 4) Working with categorical features\n",
    "\n",
    " 5) Text data transformers\n",
    "\n",
    " 6) Feature Manipulation\n",
    "\n",
    " 7) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+--------+----------------+----------------+------------+-----------+----+--------------------+--------------------+--------------------+--------+----------+--------------+------+---------+----------+---------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "| id|     timeAtServer|aircraft|        latitude|       longitude|baroAltitude|geoAltitude|numM|        measurements|       ArrayOfString|sensors_informations|serial_F| timestamp|signalstrength|serial|latitudes|longitudes|   height|     type|        sensor_infos|    All_sensor_infos|       infos_Flatten|   infos_Flatten_pad|           VectorUDT|Geohash_aircraft|           featuresc|\n",
      "+---+-----------------+--------+----------------+----------------+------------+-----------+----+--------------------+--------------------+--------------------+--------+----------+--------------+------+---------+----------+---------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "|299|0.523000001907349|       2|49.8963643736759|8.40246252111487|      2400.3|     2400.3|   3|[[440,1509552325,...|[[[440,1509552325...|[440, 1509552325,...|   440.0|1509552325|          24.0|   440|50.048573|  8.487899|   82.381|  GRX1090|[440.0, 50.048573...|[[440.0, 50.04857...|[440.0, 50.048573...|[440.0, 50.048573...|[440.0,50.048573,...|            u0vg|(66,[0,1,2,3,4,5,...|\n",
      "|615| 1.11700010299683|       2|49.8959454035355|8.40149750580659|     2407.92|    2407.92|   2|[[101,2104674703,...|[[[101,2104674703...|[101, 2104674703,...|   101.0|2104674703|          81.0|   101|50.048584|  8.487752|92.620644|Radarcape|[101.0, 50.048584...|[[101.0, 50.04858...|[101.0, 50.048584...|[101.0, 50.048584...|[101.0,50.048584,...|            u0vg|(66,[0,1,2,3,4,5,...|\n",
      "|847| 1.54299998283386|       2|49.8956660901086|8.40075518633868|     2415.54|    2415.54|   3|[[101,2524676140,...|[[[101,2524676140...|          [101,, 32]|   101.0|      null|          32.0|   101|50.048584|  8.487752|92.620644|Radarcape|[101.0, 50.048584...|[[101.0, 50.04858...|[101.0, 50.048584...|[101.0, 50.048584...|[101.0,50.048584,...|            u0vg|(66,[0,1,2,3,4,5,...|\n",
      "+---+-----------------+--------+----------------+----------------+------------+-----------+----+--------------------+--------------------+--------------------+--------+----------+--------------+------+---------+----------+---------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create features colmun\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=[\"baroAltitude\",'VectorUDT'],outputCol=\"featuresc\")\n",
    "assembler=assembler.transform(Grouped_aircarft_sensors)\n",
    "assembler.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- featuresc: vector (nullable = true)\n",
      " |-- Geohash_aircraft: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a sample :\n",
    "assembler=assembler.sample(False,0.4)\n",
    "assembler=assembler.select('featuresc', 'Geohash_aircraft')\n",
    "assembler.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           featuresc|label|\n",
      "+--------------------+-----+\n",
      "|[2400.3,440.0,50....| u0vg|\n",
      "|[2407.92,101.0,50...| u0vg|\n",
      "|[2438.4,440.0,50....| u0vg|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# I provide two other ways to build the features and labels\n",
    "# method 1 (good for small feature):\n",
    "def transData_A(row):\n",
    "    return Row(label=row['Geohash_aircraft'],features=Vectors.dense(row['baroAltitude'],row['VectorUDT']))\n",
    "# Method 2 (good for large features):\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[0]),r[-1]]).toDF(['featuresc','label'])\n",
    "transformed= transData(assembler)\n",
    "transformed.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Pipeline to deal with CategoricalCols:\n",
    "Note: You are strongly encouraged to try my **get_dummy** function for dealing with the categorical data\n",
    "in complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(66,[0,1,2,3,4,5,...| u0vg|\n",
      "|(66,[0,1,2,3,4,5,...| u0vg|\n",
      "|(66,[0,1,2,3,5,6,...| u0vg|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder,VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "     # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders]+ continuousCols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "    return data.select('features','label')\n",
    "\n",
    "catcols = []\n",
    "num_cols = ['featuresc']\n",
    "labelCol ='label'\n",
    "data = get_dummy(transformed,catcols,num_cols,labelCol)\n",
    "data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Categorical Label and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+\n",
      "|            features|label|indexedLabel|\n",
      "+--------------------+-----+------------+\n",
      "|(66,[0,1,2,3,4,5,...| u0vg|        33.0|\n",
      "|(66,[0,1,2,3,4,5,...| u0vg|        33.0|\n",
      "|(66,[0,1,2,3,5,6,...| u0vg|        33.0|\n",
      "+--------------------+-----+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Index labels, adding metadata to the label column\n",
    "#filter only right data :\n",
    "data=data.select('features','label').dropna()\n",
    "data=data.filter((data.label !=0)\n",
    "                            & (data.label!= 'null')\n",
    "                            & (data.label!= '')\n",
    "                            & (data.label != 'na')\n",
    "                            & (data.label != None)\n",
    "                             )\n",
    "data=data.dropna()\n",
    "data=data.na.drop()\n",
    "labelIndexer = StringIndexer(inputCol='label',outputCol='indexedLabel', handleInvalid=\"skip\")#options are \"keep\", \"error\" or \"skip\")\n",
    "labelIndexer.fit(data).transform(data).show(3, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all lables in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u0yh', 'u0qj', 'u10j']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labelIndexer.fit(data).labels\n",
    "labels [:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data to training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                           |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[228.6,101.0,50.048584,8.487752,43.0,92.620644,440.0,50.048573,8.487899,34.0,82.381]) |u0yh |\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[251.46,101.0,50.048584,8.487752,51.0,92.620644,440.0,50.048573,8.487899,26.0,82.381])|u0yh |\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[259.08,101.0,50.048584,8.487752,50.0,92.620644,440.0,50.048573,8.487899,27.0,82.381])|u0yh |\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                           |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[205.74,101.0,50.048584,8.487752,41.0,92.620644,440.0,50.048573,8.487899,41.0,82.381])|u0yh |\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[251.46,101.0,50.048584,8.487752,47.0,92.620644,440.0,50.048573,8.487899,24.0,82.381])|u0yh |\n",
      "|(66,[0,1,2,3,4,5,6,7,8,9,10],[259.08,101.0,50.048584,8.487752,96.0,92.620644,440.0,50.048573,8.487899,6.0,82.381]) |u0yh |\n",
      "+-------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.8, 0.2])\n",
    "trainingData.show(3,False)\n",
    "testData.show(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaiveBayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pipeline Architecture:\n",
    "  ###### Preporcessing Data+Model +label converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "labelIndexer = StringIndexer(inputCol='label',outputCol='indexedLabel', handleInvalid=\"skip\")\n",
    "\n",
    "# Let us create an object of MinMaxScaler class\n",
    "MinMaxScaler=MinMaxScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "# Train a GBT model.\n",
    "\n",
    "nb =NaiveBayes(labelCol=\"indexedLabel\", featuresCol=\"Scaled_features\")\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=labels)\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline2 = Pipeline(stages=[ labelIndexer,MinMaxScaler ,nb,labelConverter])\n",
    "# Train model\n",
    "model2 = pipeline2.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------+\n",
      "|            features|label|predictedLabel|\n",
      "+--------------------+-----+--------------+\n",
      "|(66,[0,1,2,3,4,5,...| u0yh|          u0yh|\n",
      "|(66,[0,1,2,3,4,5,...| u0yh|          u0yh|\n",
      "|(66,[0,1,2,3,4,5,...| u0yh|          u0yh|\n",
      "|(66,[0,1,2,3,4,5,...| u0yh|          u0yh|\n",
      "|(66,[0,1,2,3,4,5,...| u0yh|          u0yh|\n",
      "+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model2.transform(testData)\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.979011\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.0209893\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %g\" % (accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayes_b25a191d6a55\n"
     ]
    }
   ],
   "source": [
    "rfModel = model2.stages[-2]\n",
    "print(rfModel) # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
