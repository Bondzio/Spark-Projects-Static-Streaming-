{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured streaming in Pyspark \n",
    "Everybody talks streaming nowadays – social networks, online transactional systems they all generate data. Data collection means nothing without proper and on-time analysis. In this new data age, we are privileged with the right tools to make the best use of our data. We can use structured streaming to take advantage of this and act quickly upon new trends, this could bring to insights unseen before.\n",
    "Spark offers two ways of streaming:\n",
    "• Spark Streaming\n",
    "• Structured streaming (officially introduced with Spark 2.0, production-ready with Spark 2.2)\n",
    "Let’s add a few words for both streaming options below.\n",
    "# Spark Streaming\n",
    "Spark Streaming is a separate library in Spark which provides a basic abstraction layer called Discretized Stream or DStream, it processes continuously flowing streaming data by breaking it up into discrete chunks. DStream is the original, RDD (Resilient Distributed Dataset) based streaming API for Spark.\n",
    "Spark streaming has the following problems:\n",
    "• Difficult – not simple to build streaming pipelines which support late data or fault tolerance. All of them are achievable but they need some extra development work.\n",
    "• Inconsistent Integration- API used to generate batch processing (RDD, Dataset) is different than the API of streaming processing (DStream).\n",
    "• Processing order – later generated data is processed before earlier generated data.\n",
    "# Structured Streaming\n",
    "Structured streaming is based on Dataframe and Dataset APIs, it is easier to implement and SQL queries are easily applied. Most importantly, Structured streaming incorporates the following features:\n",
    "• Strong guarantees about consistency with batch jobs – the engine uploads the data as a sequential stream.\n",
    "• Transactional integration with storage systems – transactional updates are part of the core API now, once data is processed it is only being updated, this provides a consistent snapshot of the data.\n",
    "• Tight integration with the rest of Spark – Structured Streaming supports serving interactive queries on streaming state with Spark SQL and JDBC, and integrates with MLlib.\n",
    "• Late data support – explicit support of “event time” to aggregate out of order data (late data) and bigger support for windowing and sessions, this avoids the problems Spark Streaming has with Processing Order.\n",
    "# Watermarking\n",
    "\n",
    "Firstly we will introduce a Watermark in order to handle late arriving data. Spark’s engine automatically tracks the current event time and can filter out incoming messages if they are older than time T. In our use case we want to filter out messages that just arrived but are more than 1 day old. We can do that with the following code\n",
    "# output modes:\n",
    "\n",
    "    Append mode - Only the new rows added to the Result Table since the last trigger will be outputted to the sink\n",
    "    Complete Mode - The whole Result Table will be outputted to the sink after every trigger (only supported for aggregate queries)\n",
    "    Update Mode - Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink\n",
    "    \n",
    "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking\n",
    "# Example\n",
    "# Source Data\n",
    "For this tutorial I’ve used open-source data for taxis in NYC:\n",
    "https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "The purpose of this blog is to introduce you to structured streaming using a simple scenario with CSV input data with specific dates and times which we will be able to use. The static set of files are used to emulate streaming taxi orders. I computed real-time metrics like peak time of taxi pickups and drop-offs, most popular boroughs for taxi demand.\n",
    "For this exercise, I took one FHV Taxi CSV file – for January 2018 and split it into multiple smaller sized files.\n",
    "The data is 11 CSV files – 10 with transactional records and one location mapping, each transactional CSV file has about 5000 rows. The collection of the files serve as an echo of what real data might be like.\n",
    "I’ve done some testing in terms of storage – if you decide to use ADLS the structured streaming won’t be working since it requires uploading of new files in the streaming folder or editing while the streaming is on. \n",
    "For the below test I chose DBFS as a storage location.\n",
    "All source files and Databricks notebook can be found on the following link:\n",
    "https://github.com/VickyAugust10/PysparkStructuredStreaming\n",
    "Shall we get started?\n",
    "Let’s get a preview of our main source folder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()#create spark session \n",
    "sc = spark.sparkContext#create sparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "# from pyspark.sql.functions import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as func\n",
    "import matplotlib.patches as mpatches\n",
    "from operator import add\n",
    "from operator import add\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import itertools\n",
    "from pyspark.streaming import StreamingContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Path to our 20 JSON files\n",
    "\n",
    "inputPath = \"C:/Users/rzouga/Downloads/Work Swiss/location-exploration/Recruitment-Challenge/streaming/\"\n",
    "# Explicitly set schema\n",
    "schema = StructType([ StructField(\"time\", TimestampType(), True),\n",
    "                      StructField(\"customer\", StringType(), True),\n",
    "                      StructField(\"action\", StringType(), True),\n",
    "                      StructField(\"device\", StringType(), True)])\n",
    "from pyspark.sql.types import TimestampType, StringType, StructType, StructField\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame representing data in the JSON files\n",
    "inputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(schema)\n",
    "    .json(inputPath)\n",
    ")\n",
    "\n",
    "# Remove empty rows\n",
    "inputDF = inputDF.dropna()\n",
    "\n",
    "# Aggregate number of actions\n",
    "actionsDF = (\n",
    "  inputDF\n",
    "    .groupBy(\n",
    "       inputDF.action\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "actionsDF.cache()\n",
    "\n",
    "# Create temp table named 'iot_action_counts'\n",
    "actionsDF.createOrReplaceTempView(\"iot_action_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create streaming equivalent of `inputDF` using .readStream\n",
    "streamingDF = (\n",
    "  spark\n",
    "    .readStream\n",
    "    .schema(schema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .json(inputPath)\n",
    ")\n",
    "# Stream `streamingDF` while aggregating by action\n",
    "streamingActionCountsDF = (\n",
    "  streamingDF\n",
    "    .groupBy(\n",
    "      streamingDF.action\n",
    "    )\n",
    "    .count()\n",
    ")\n",
    "# Is `streamingActionCountsDF` actually streaming?\n",
    "streamingActionCountsDF.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.streams.active "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used for testing \n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "# View stream in real-time\n",
    "query = (\n",
    "  streamingActionCountsDF.writeStream.queryName(\"streamingActionCountsDF\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have a streaming dataframe, but it is not writing anywhere. We need to stream to a certain destination using writestream() on our dataframe with concrete options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|action|count|\n",
      "+------+-----+\n",
      "+------+-----+\n",
      "\n",
      "+-----------+-----+\n",
      "|     action|count|\n",
      "+-----------+-----+\n",
      "|       null|    1|\n",
      "|low battery|  339|\n",
      "|  power off|  347|\n",
      "+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|     action|count|\n",
      "+-----------+-----+\n",
      "|       null|    2|\n",
      "|low battery|  668|\n",
      "|  power off|  678|\n",
      "+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|     action|count|\n",
      "+-----------+-----+\n",
      "|       null|    3|\n",
      "|low battery|  986|\n",
      "|  power off| 1019|\n",
      "+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|     action|count|\n",
      "+-----------+-----+\n",
      "|       null|    4|\n",
      "|low battery| 1312|\n",
      "|  power off| 1357|\n",
      "+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"select * from streamingActionCountsDF\").show(3)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# View stream in real-time\\nquery = (\\n  streamingActionCountsDF\\n    .writeStream\\n    .format(\\'console\\')    .outputMode(\"complete\")    .start()\\n)\\n\\nquery.awaitTermination()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this used for testing also \n",
    "\n",
    "\"\"\"# View stream in real-time\n",
    "query = (\n",
    "  streamingActionCountsDF\n",
    "    .writeStream\n",
    "    .format('console')\\# parquet for real program \n",
    "    .outputMode(\"complete\")\\\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "query = (\n",
    "  OrderByBoroughPerDayAndServiceType\n",
    "    .writeStream\n",
    "    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n",
    "    .queryName(\"counts\")     # counts = name of the in-memory table            \n",
    "    .outputMode(\"complete\")  # complete = all the counts should be in the table\n",
    "    .start()\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= locations.writeStream\\\n",
    "  .format('console')\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string, count: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"StructuredNetworkWordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "wordCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordCounts.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Start running the query that prints the running counts to the console\n",
    "query1 =(wordCounts.writeStream.queryName(\"wordCounts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start())\n",
    "\n",
    "from time import sleep\n",
    "for x in range(2):\n",
    "    spark.sql(\"select * from wordCounts\").show(2)\n",
    "    sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://fr.slideshare.net/databricks/deep-dive-into-stateful-stream-processing-in-structured-streaming-with-tathagata-das?from_action=save\n",
    "# http://www.dcs.bbk.ac.uk/~dell/teaching/cc/book/databricks/spark-intro.pdf\n",
    "#https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html\n",
    "#https://databricks.com/blog/2017/04/26/processing-data-in-apache-kafka-with-structured-streaming-in-apache-spark-2-2.html\n",
    "# https://sparkbyexamples.com/category/spark/apache-spark-streaming/\n",
    "# https://hackersandslackers.com/structured-streaming-in-pyspark/\n",
    "#https://www.toptal.com/apache/apache-spark-streaming-twitter\n",
    "#https://stackoverflow.com/questions/45411285/spark-structured-streaming-and-filters\n",
    "# https://adatis.co.uk/structured-streaming-in-pyspark-using-databricks-2/\n",
    "# https://github.com/h2oai/sparkling-water/tree/master/examples#step-by-step-through-weather-data-example\n",
    "# https://blog.invivoo.com/structured-streaming-in-spark/\n",
    "# https://github.com/falaybeg/SparkStreaming-Network-Anomaly-Detection/blob/master/k-means_network-anomaly.ipynb\n",
    "#https://gist.github.com/rmoff/eadf82da8a0cd506c6c4a19ebd18037e\n",
    "# https://databricks.com/blog/2017/04/04/real-time-end-to-end-integration-with-apache-kafka-in-apache-sparks-structured-streaming.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
